{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1e2096",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning and Cross-Validation #\n",
    "\n",
    "The purpose of this notebook is to introduce various means of hyperparameter tuning and cross-validation.\n",
    "\n",
    "### Hyperparameter Tuning ###\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal configuration for a machine learning model. It involves testing different values of hyperparameters for a given ML algorithm and selecting the combination that maximizes performance.\n",
    "\n",
    "There are different techniques for hyperparameter tuning, many of which are built into machine learning modules like SKLearn. Some common techniques are covered in this notebook:\n",
    "* Grid Search - Exhaustively evaluates all possible hyperparameter combinations.\n",
    "* Randomized Search - A faster version of Grid Search that samples random combinations of hyperparameters.\n",
    "* Bayesian Optimization - Uses probabilistic models to find optimal hyperparameters more efficiently.\n",
    "\n",
    "### Cross-Validation ###\n",
    "\n",
    "Cross-validation is a technique to minimize overfitting and it is especially important with regard to hyperparameter tuning. The basic idea is to create many different sets of training data and to evaluate the model's cumulative performance.\n",
    "\n",
    "There are different techniques for cross-validation, many of which are built into machine learning modules like SKLearn. Some common techniques are covered in this notebook:\n",
    "* Leave-P-Out (see also Leave-One-Out) - Removes `p` samples for validation in each iteration.\n",
    "* Stratified K-Fold - Ensures that each fold maintains a balance when there are common vs rare classification labels.\n",
    "* Shuffle-Split - Randomly partitions data into multiple train-test splits.\n",
    "\n",
    "By using cross-validation, we ensure that our chosen hyperparameters generalize well to unseen data, improving the model's robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV, \n",
    "    LeavePOut, StratifiedKFold, ShuffleSplit)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# pip install scikit-optimize\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceaa7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff60a50c",
   "metadata": {},
   "source": [
    "## Wine Dataset ##\n",
    "\n",
    "We are going to reuse the wine dataset. It's not so much that I particularly enjoy this dataset, but it's a nice small size for this lesson and all of the values are numeric. Notice that I skip over any sort of normalization or standardization. It might be possible to improve scores by taking these sorts of things into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df[\"target\"] = wine.target\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c40e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87550a41",
   "metadata": {},
   "source": [
    "### Validation Sets ###\n",
    "\n",
    "We need to set aside a portion of the data to use as our ***validation set***. Even though we are going to be using cross-validation to cycle through the data, it is important to still set aside a portion of the data for final testing. In this way, we have actually have three sets of data. The first two come from the 'training' data and the last one comes from the 'test' data.\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, ...)\n",
    "```\n",
    "\n",
    "#### X_train, y_train ####\n",
    "This data is used for the cross-validation. It will be split over-and-over again to create different sets of training and test data.\n",
    "* Training set: the true training data, a different subset of X_train and y_train for each cross-validation set\n",
    "* Validation set: intermediate \"test\" data, a different subset of X_train and y_train for each cross-validation set\n",
    "\n",
    "#### X_test, y_test ####\n",
    "* Test set: this data is the true test data; it is held aside from the very beginning for final score. We will not use this test data for any of the cross-validation models because that could lead to data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382dcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e8e832b",
   "metadata": {},
   "source": [
    "## Two New ML Algorithms ##\n",
    "\n",
    "We are going to try two new machine learning classification algorithms, Support Vector Machines (SVM) and Gradient Boost. Let's check in with ChatGPT for a description of these algorithms:\n",
    "\n",
    "### Support Vector Machines ###\n",
    "\n",
    "\"Support Vector Machines (SVM) are supervised learning algorithms used for classification and regression tasks. SVM works by finding the optimal hyperplane that best separates different classes in a dataset. It maximizes the margin (distance between the closest data points, called support vectors) to ensure better generalization. When the data is not linearly separable, SVM uses the kernel trick (e.g., polynomial or RBF kernel) to transform data into a higher-dimensional space where a linear boundary can be applied. SVMs are particularly effective in high-dimensional spaces and small datasets, but they can be computationally expensive for large datasets.\"\n",
    "\n",
    "### Gradient Boost ###\n",
    "\n",
    "\"Gradient Boosting is a machine learning method that builds a strong model by combining many weak models (usually small decision trees). It works step by step, where each new tree tries to fix the mistakes made by the previous trees. Instead of treating all mistakes equally, Gradient Boosting focuses more on errors that were hardest to correct. By doing this repeatedly, the model improves over time. This method is very powerful for complex, non-linear problems, but it needs careful tuning to avoid overfitting (memorizing the training data too much). It is commonly used in applications like fraud detection, ranking systems, and predicting customer behavior.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e933cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"SVM\": SVC(random_state=432),\n",
    "    \"Gradient Boost\": GradientBoostingClassifier(random_state=432)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3796dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83f79dd5",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning ###\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal configuration for a machine learning model. It involves testing different values of hyperparameters for a given ML algorithm and selecting the combination that maximizes performance.\n",
    "\n",
    "For example, in clustering, the K-means algorithm requires the user to choose the number of clusters. We used WSSE elbow plots to find the optimum value for `k`. Similarly, the DBSCAN algorithm is depends on `min_samples` (minimum number of neighbors) and `eps` (neighborhood size). We combined the Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index metrics to optimize these parameters.\n",
    "\n",
    "In regression learning, we might choose a polynomial model that requires us to specify options like the polynomial degree and learning rate.\n",
    "\n",
    "And finally, with classification algorithms like K-Nearest Neighbors, the user is able to choose between various `metric` values (e.g., `\"euclidean\"` or `\"manhattan\"`) and a voting strategy using the `weights` parameter.\n",
    "\n",
    "In each of these cases, selecting the optimal hyperparameter values can significantly impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will compare Gradient Boost and Support Vector Machine, two new ML algorithms\n",
    "param_grids = {\n",
    "    'Gradient Boost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f1f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da9f5378",
   "metadata": {},
   "source": [
    "### Cross-Validation ###\n",
    "\n",
    "Cross-validation is a technique to minimize overfitting and it is especially important with regard to hyperparameter tuning. The basic idea is to create many different sets of training data and to evaluate the model's cumulative performance.\n",
    "\n",
    "Think back to how we performed hyperparameter tuning. We basically tried a bunch of different parameter values and then found which combination gave the highest accuracy score. One of the potential problems with this approach is that it is prone to overfitting. The best parameters are only \"best\" for the chosen train-test split. A different set of training data might have led us to choose different hyperparameters. This happens because the model's performance may vary depending on the data split, leading to inconsistent hyperparameter selection.\n",
    "\n",
    "Cross-validation mitigates this by repeatedly training and evaluating the model on different train-test splits, producing a more reliable estimate of model performance. The final evaluation metric is averaged over multiple train-test splits, providing a more reliable estimate of the model's true performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c40ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three cross-validation techniques\n",
    "cv_methods = {\n",
    "    \"Shuffle-Split\": ShuffleSplit(n_splits=10, test_size=0.2, random_state=123),\n",
    "    \"Stratified K-Fold\": StratifiedKFold(n_splits=5, shuffle=True, random_state=123),\n",
    "    #\"Leave-P-Out\": LeavePOut(p=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474cd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae80b4a",
   "metadata": {},
   "source": [
    "## Evaluating the Models ##\n",
    "\n",
    "The final step is to create and validate (test) all of the models, keeping track of the best performers.\n",
    "\n",
    "We're going to find that the wine dataset is relatively small and each class is fairly easy to predict, so the accuracy of each cross-validation method will be roughly the same, though there may be some small difference in the values. We had to keep the dataset simple in order for the cross-validation techniques to finish in a reasonable time on my little laptop. Had we more processing power, we could have analyzed a larger and more complex dataset, where the differences would be more pronounced.\n",
    "\n",
    "The key to notice here is the differnce in execution time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform hyperparameter tuning with different cross-validation methods\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    for cv_name, cv_method in cv_methods.items():\n",
    "        print(f\"  Using {cv_name} cross-validation\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grids[model_name], \n",
    "            cv=cv_method, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        print(f\"    Grid Search  ({end_time - start_time:5.2f}s): {accuracy_score(y_test, y_pred):.2f} accuracy with {grid_search.best_params_}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        random_search = RandomizedSearchCV(\n",
    "            model, param_grids[model_name], \n",
    "            cv=cv_method, scoring='accuracy', \n",
    "            n_iter=5, random_state=17, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        y_pred = random_search.best_estimator_.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        print(f\"    Rnd Search   ({end_time - start_time:5.2f}s): {accuracy_score(y_test, y_pred):.2f} accuracy with {random_search.best_params_}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        bayes_search = BayesSearchCV(\n",
    "            model, param_grids[model_name], \n",
    "            cv=cv_method, scoring='accuracy', \n",
    "            n_iter=10, random_state=17, n_jobs=-1)\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        y_pred = bayes_search.best_estimator_.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        print(f\"    Bayes Search ({end_time - start_time:5.2f}s): {accuracy_score(y_test, y_pred):.2f} accuracy with {bayes_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae24576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "334d3f62",
   "metadata": {},
   "source": [
    "### Analyzing Results ###\n",
    "\n",
    "We can dig deeper into the results by comparing the accuracy of each test using the `cv_results_` parameter. There are several different metrics available, mostly related to accuracy score and execution time. We will look into the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46290dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_search.cv_results_['params'])\n",
    "print(random_search.cv_results_['mean_test_score'])\n",
    "print(random_search.cv_results_['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b06e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(random_search.cv_results_['params'], random_search.cv_results_['mean_test_score'])\n",
    "combined = sorted(list(combined), key=lambda x: x[1], reverse=True)\n",
    "for param, score in combined:\n",
    "    print(f\"Accuracy {100*score:.3f}%: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(grid_search.cv_results_['params'], grid_search.cv_results_['mean_test_score'])\n",
    "combined = sorted(list(combined), key=lambda x: x[1], reverse=True)\n",
    "for param, score in combined:\n",
    "    print(f\"Accuracy {100*score:.3f}%: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37395ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf3f4878",
   "metadata": {},
   "source": [
    "## Final Testing ##\n",
    "\n",
    "Now that we have selected the best model, it's time to make predictions on our real test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(random_state=432, kernel='linear', C=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report for Support Vector Machine on Wine Dataset\")\n",
    "print(\"Predicting originating vineyard based on chemical composition of wine\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4235b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae2737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
