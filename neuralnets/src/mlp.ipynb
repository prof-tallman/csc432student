{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1e2096",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Scikit-Learn vs PyTorch, Keras, and TensorFlow #\n",
    "\n",
    "Today's topic is to learn how to use the PyTorch, Keras, and TensorFlow frameworks. These three machine learning packages are built specifically for neural networks and deep learning.\n",
    "\n",
    "### Scikit-Learn ###\n",
    "\n",
    "Our old ML framework, Scikit-Learn, is great for classical ML algorithms and building shallow networks. However, it has significant limitations for modern neural networks such as CNNs, RNNs, and Transformers. And there is no support for GPU acceleration (contrary to what I said in class) *unless* you install NVIDIA's RAPIDS cuML package `import cuml` or Intel's OneAPI package `import sklearnex`. Each of these packages is linked directly to your graphics card, so the cuML module won't help if you have an Intel GPU and vice-versa.\n",
    "\n",
    "### PyTorch, Keras, and TensorFlow ###\n",
    "\n",
    "In contrast, these new frameworks are built for deep learning. PyTorch was originally created as an open-source project by Meta (Facebook) but at this point the development is mostly driven by the larger ML/AI community. TensorFlow is an open-source Google product. Although anyone can theoretically contribute to it, the project is largely under control of Google AI and it has built-in support for processing on Google Cloud AI.\n",
    "\n",
    "All three deep-learning frameworks provide native GPU acceleration. PyTorch requires the user to manually enable the GPU whereas the other two frameworks offload large computations to the GPU automatically.\n",
    "\n",
    "Keras used to be its own independent, high-level deep-learning package. It could run on top of a handful of various low-level packages, but the most common was Google's TensorFlow. In 2019, Google integrated Keras into TensorFlow. If you want create models at a high-level, taking the default architectures, then you use Keras `from tensorflow import keras`. But if you want low-level customization, then you can use TensorFlow on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Kaggle's \"Give Me Some Credit\" Dataset ##\n",
    "\n",
    "The purpose of this dataset is to predict which loan applications should be denied. The positive case (1) is rejecting the loan and the negative case (0) is approving it. There are some 150,000 samples to train a model and test with. Each sample is described by 10 features that have to do with things like monthly income, number of loans, number of delinquent payments, credit utilization (how much of your approved credit have you actually used), etc, etc. The dataset is missing relatively few values, but you'll need to analyze it and decide how to fix these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the credit dataset\n",
    "df = pd.read_csv('credit_training.csv').drop(columns=['Unnamed: 0'])\n",
    "df = df.rename(columns={'RevolvingUtilizationOfUnsecuredLines':'CreditUtilization',\n",
    "                        'NumberOfTime30-59DaysPastDueNotWorse':'PastDue30-59',\n",
    "                        'NumberOfTime60-89DaysPastDueNotWorse':'PastDue60-89',\n",
    "                        'NumberOfTimes90DaysLate':'PastDue90+',\n",
    "                        'NumberOfOpenCreditLinesAndLoans':'CreditLines',\n",
    "                        'NumberRealEstateLoansOrLines':'RealEstateLoans',\n",
    "                        'NumberOfDependents':'Dependents',\n",
    "                        'age':'Age',\n",
    "                        'SeriousDlqin2yrs':'RejectLoan'})\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Cleanup ###\n",
    "\n",
    "Let's peruse the data and then handle any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['RejectLoan'])\n",
    "y = df['RejectLoan']\n",
    "print(f\"Features: {X.columns.to_list()[:5]}...\")\n",
    "print(f\"Output: '{y.name}'\")\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=432, stratify=y)\n",
    "print(f\"Training count: {X_train.shape}\")\n",
    "print(f\"Test count:     {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we missing any values? Yes: MonthlyIncome and Dependents\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate 'Dependents' first, it seems the easier\n",
    "X_train['Dependents'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf16e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save value for later; then fix and confirm\n",
    "dependents_na_value = X_train['Dependents'].mean()\n",
    "X_train['Dependents'] = X_train['Dependents'].fillna(dependents_na_value)\n",
    "X_train['Dependents'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c234da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's investigate monthly income... wow, more variance\n",
    "X_train['MonthlyIncome'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does NaN mean the applicant doesn't have a job? \n",
    "# There are lots of samples (1250+) with a value of 0\n",
    "X_train[X_train['MonthlyIncome'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating more, it looks like the DebtRatio is high, but this implies they have some income\n",
    "# Let's fill NaNs with the median value\n",
    "X_train[X_train['MonthlyIncome'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save value for later; then fix and confirm\n",
    "income_na_value = X_train['MonthlyIncome'].median()\n",
    "X_train['MonthlyIncome'] = X_train['MonthlyIncome'].fillna(income_na_value)\n",
    "X_train['MonthlyIncome'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification... remember we have two fills to do for any prediction\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb167d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering ###\n",
    "\n",
    "Neural networks usually perform better with standardized data because certain activation functions are sensitive to outliers and the gradient descent algorithm converges more efficiently when inputs have similar scales. So at this point, let's just standardize everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3214b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn MLP Model ###\n",
    "\n",
    "Let's start with a model that we already know, a simple Scikit-Learn MLP neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64,32),    # read somewhere that this was reasonable complexity\n",
    "    activation='tanh',             # 'relu', 'logistic', 'tanh', 'identity'\n",
    "    solver='sgd',                  # 'adam' or 'sgd' or 'lbfgs' ...usually adam is best for general case\n",
    "    max_iter=500,\n",
    "    random_state=123,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.01\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e58ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the prediction data the same way that we did the training data\n",
    "X_test['Dependents'] = X_test['Dependents'].fillna(dependents_na_value)\n",
    "X_test['MonthlyIncome'] = X_test['MonthlyIncome'].fillna(income_na_value)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make predictions and look at some performance metrics\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred)}\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_probs = mlp.predict_proba(X_test)\n",
    "y_probs_0 = y_probs[:,0] # test probabilities that were classified as 'approve' (0)\n",
    "y_probs_1 = y_probs[:,1] # test probabilities that were classified as 'reject' (1)\n",
    "print(f\"Sample results for predicted 'approve': {y_probs_0[:5]}\")\n",
    "print(f\"Sample results for predicted 'reject':  {y_probs_1[:5]}\")\n",
    "\n",
    "# Bring back the confusion matrix to quickly see TP/TN/FP/FN\n",
    "class_names = [\"Approve\", \"Reject\"]\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(f\"Confusion Matrix (Threshold = 0.50)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC) Curve ###\n",
    "\n",
    "The ROC Curve is a new, visual evaluation technique. It works for models that output probabilities and is well suited for binary classification problems. It allows us to compare the tradeoff between the True Positive Rate (which you already know as *recall*) and the False Positive Rate.\n",
    "\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "FPR = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "The two metrics are somewhat related and the ROC curve shows us how certain tweaks to the model will affect its TPR and FPR. The left-most corner of the ROC Curve is considered the optimum level because it maximizes the difference between the TPR and the FPR. In general, we want to maximize TPR and minimize FPR (obviously), but there are situations where one metric might be preferred over the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# This calculates the FPR and TPR at a variety of threshold levels\n",
    "# The three ROC Curve variables are all parallel lists\n",
    "#   i.e., at this particular treshold level, these are the FPs and the TPs\n",
    "y_probs = mlp.predict_proba(X_test)\n",
    "y_probs_0 = y_probs[:,0] # test probabilities that were classified as 'approve' (0)\n",
    "y_probs_1 = y_probs[:,1] # test probabilities that were classified as 'reject' (1)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_1)\n",
    "\n",
    "# Find the optimal threshold (closest to the top-left corner of the ROC curve, i.e., TPR = 1, FPR = 0)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "# Plot ROC Curve\n",
    "# The optimal threshold is not always 0.5.\n",
    "# Lower values (~0.3) favor recall (catch more positives, but increase false alarms).\n",
    "# Higher values (~0.7) favor precision (reduce false positives, but miss some positives).\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', zorder=1)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve', zorder=2)\n",
    "\n",
    "# Annotate a few threshold points and highlight the optimal threshold\n",
    "# ChatGPT's help\n",
    "for i in range(0, len(thresholds), max(1, len(thresholds) // 5)):  \n",
    "    plt.annotate(f\"{thresholds[i]:.2f}\", (fpr[i], tpr[i]), textcoords=\"offset points\", xytext=(5, -5), ha='left', zorder=4)\n",
    "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, marker='x', label=f'Optimal Threshold = {optimal_threshold:.2f}', zorder=5)\n",
    "plt.annotate(f\"{optimal_threshold:.2f}\", (fpr[optimal_idx], tpr[optimal_idx]), textcoords=\"offset points\", xytext=(-20, 10), ha='right', color='red', zorder=6)\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve with Optimal Threshold\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Thresholds ###\n",
    "\n",
    "True Positive Rate and False Postivie Rate are related to each other and we can change them by setting a custom threshold for our model. By default, a classifier will choose the positive outcome if the final output is >= 0.50 and the negative outcome if it is < 0.50. Adjusting the classification threshold affects both TPR and FPR. Lowering the threshold increases TPR but often at the cost of a higher FPR, and vice versa.\n",
    "\n",
    "For example, with the credit application dataset, if we want to minimize false negatives--predicting approval but the borrower actually defaults--then we need to lower the threshold value so that more predictions are categorized as the positive case (reject). We might decide that all outputs >= 0.30 should be rejected.\n",
    "\n",
    "Whenever we change the threshold, we favor one type of prediction over the other (i.e., error on the side of... ). For example, lowering the threshold increases true positives but also increases false positives. Choosing an optimal threshold depends on the problem and whether we want to prioritize sensitivity (catching all positive cases--*recall*) or specificity (avoiding false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the prediction using the custom threshold level\n",
    "# Note that we do *not* call predict() again, we just do a custom classification step\n",
    "# using the output probabilities from the original model.\n",
    "custom_threshold = 0.06\n",
    "y_probs = mlp.predict_proba(X_test)\n",
    "y_probs_0 = y_probs[:,0] # test probabilities that were classified as 'approve' (0)\n",
    "y_probs_1 = y_probs[:,1] # test probabilities that were classified as 'reject' (1)\n",
    "y_pred_custom = (y_probs_1 >= custom_threshold).astype(int)\n",
    "\n",
    "# Evaluate accuracy\n",
    "# Notice all of the 'y_pred' have been changed to 'y_pred_custom' \n",
    "# Otherwise, the code is exactly the same as before\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred_custom)}\\n\")\n",
    "print(classification_report(y_test, y_pred_custom))\n",
    "y_probs = mlp.predict_proba(X_test)\n",
    "y_probs_0 = y_probs[:,0] # test probabilities that were classified as 'approve' (0)\n",
    "y_probs_1 = y_probs[:,1] # test probabilities that were classified as 'reject' (1)\n",
    "print(f\"Sample results for predicted 'approve': {y_probs_0[:5]}\")\n",
    "print(f\"Sample results for predicted 'reject':  {y_probs_1[:5]}\")\n",
    "\n",
    "# Bring back the confusion matrix to quickly see TP/TN/FP/FN\n",
    "class_names = [\"Approve\", \"Reject\"]\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_custom), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(f\"Confusion Matrix (Threshold = 0.50)\")\n",
    "plt.show()\n",
    "\n",
    "# Is this better? It depends on the application.\n",
    "# Notice that our accuracy has dropped significantly but we a lot fewer false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under Curve (AUC) Score ###\n",
    "\n",
    "By changing the threshold value, we can bias the model towards the negative case (lower threshold) or the positive case (higher threshold). But nothing in the model has changed--it's still predicting the exact same values for every input. Changing the threshold value merely moves our model back and forth along the ROC Curve. So wouldn't it be nice to have a single number that would help us evaluate a model, regardless of the desired threshold?\n",
    "\n",
    "There is such a metric: the Area Under Curve Score. The AUC Score is exactly as it sounds. It is the integral of the ROC Curve. The ideal ROC curve is like an upside down elbow. The y-value rises very, very quickly and then holds steady. Such a graph would indicate that we have a high true positive rate and a low false positive rate no matter the threshold value. It would mean that the optimal threshold value is close to the point (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Notice that I ignored the the test probabilities classified as 0... this is standard\n",
    "y_probs = mlp.predict_proba(X_test)[:,1]\n",
    "print(f\"Overall AUC Score for model: {roc_auc_score(y_test, y_probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
